---
title: "Tweets about Mueller Investigation"
author: "mwk"
date: "2017-11-02"
slug: tweets-about-mueller-investigation
categories: ["R"]
tags: ["politics", "rtweet"]
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, collapse = TRUE)
```

On Monday (Oct. 30th), the [first charges were
filed](http://www.cnn.com/2017/10/27/politics/first-charges-mueller-investigation/index.html)
in the Mueller investigation on potential connections between the
Trump campaign and the Kremlin. It's major political news, so I wanted
to capture as much of the relevant tweets as I could. In this post,
I've documented the step-by-step of how I went about collecting the
data.

### 1. Install and load rtweet

Although I expect to update rtweet on CRAN very soon, the code below
assumes the installed version of rtweet is **at least** version
`'0.5.32'`, which---at the time of writing---is only available on
Github. So, first install [rtweet from
Github](https://github.com/mkearney/rtweet) and then load the package.

```{r}
## install devtools if necessary
if (!requireNamespace("devtools", quietly = TRUE)) {
  install.packages("devtools")
}

## install dev version of rtweet from github
devtools::install_github("mkearney/rtweet")

## load rtweet
library(rtweet)
```

### 2. Search for tweets

Next, search for tweets containing one or more keywords. I selected
the important names from Monday's news stories. To search for **any**
occurrance, make sure to separate each search term by ` OR `. The code
below asks for 4 million tweets containing at least one of the queried
search terms. To automate the process of dealing with Twitter rate
limits---which is required for a single `search_tweets()` call to
return more than 18,000 tweets---set `retryonratelimit =
TRUE`. Executing this code will take around 55.5 hours, but otherwise
it's super easy!

```{r}
## search for 4 million tweets mentioning any of these keywords
rt <- search_tweets(
  "papadopolous OR gates OR mueller OR manafort",
  n = 4e6,
  retryonratelimit = TRUE
)
```

### 3. Save the data

I recommend saving the data as an R data file via the `saveRDS()` function.

```r}
## save
saveRDS(rt, "~/Dropbox/mueller-investigation.rds")
```

### 4. Visualize the data

The remaining code creates a dummy variable for which key figure was
mentioned by tweets, groups the data by that created variable, and
then plots the frequencies of those grouped data over time.

```{r}
## user defined function to create grouping var with multiple levels
mkp <- function(m, x) {
  kp <- grepl(m, x$text, ignore.case = TRUE)
  usr <- users_data(x)
  usr <- usr[usr$user_id %in% x$user_id[kp], ]
  x <- x[kp, ]
  x$regmatch <- m
  attr(x, "users") <- usr
  x
}

## apply grouping fun to data rt
rtm <- lapply(
  c("mueller", "gates", "papadopoulos", "manafort"),
  mkp, rt
)

## handy new rtweet function for binding a list of tweets/users data
rtm <- do_call_rbind(rtm)

## load tidyverse
library(tidyverse)

## group using regmatch variable and convert into time series like data
rts <- rtm %>%
  group_by(regmatch) %>%
  ts_data("5 mins", trim = 1L)

## create ggplot
rts %>%
  mutate(
    regmatch = factor(regmatch, labels = c("Gates", "Manafort", "Mueller", "Papadopolous"))) %>%
  ggplot(aes(x = time, y = n, colour = regmatch)) +
  geom_line() +
  ## a ggplot2 theme from my own utility package
  tfse::theme_mwk() +
  labs(
    x = NULL, y = NULL,
    title = "Twitter statuses about recent charges in Mueller's Russia investigation",
    subtitle = "Search terms–Gates, Manafort, Mueller, Papadopolous–aggregated in 5-minute intervals",
    caption = "\nSource: Data (N = 3,149,735) collected from Twitter's search API via rtweet"
  ) +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = rel(.75)),
    plot.caption = element_text(colour = "gray30", size = rel(.65))
  ) +
  guides(colour = guide_legend(override.aes = list(size = 0.725))) +
  ggsave("~/mueller.png", width = 10, height = 7, unit = "in")
```

And here's what the plot looks like:

![](../mueller.png)
